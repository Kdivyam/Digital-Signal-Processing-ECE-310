{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification (Part 2) and Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "from sklearn import neighbors\n",
    "from sklearn import svm\n",
    "from sklearn import model_selection\n",
    "from numpy import genfromtxt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spam Detection \n",
    "\n",
    "In this problem, I constructed a crude spam detector. As you all know, when you receive an e-mail, it can be divided into one of two types: ham (useful mail, label $-1$) and spam (junk mail, label $+1$). In the [olden days](http://www.paulgraham.com/spam.html), people tried writing a bunch of rules to detect spam. However, it was quickly seen that machine learning approaches work fairly well for a little bit of work. \n",
    "\n",
    "I designed a spam detector by applying some of the classification techniques you learned in class to a batch of emails used to train and test [SpamAssassin](http://spamassassin.apache.org/), a leading anti-spam software package. \n",
    "\n",
    "Let the *vocabulary* of a dataset be a list of all terms occuring in a data set. So, for example, a vocabulary could be [\"cat\",\"dog\",\"chupacabra\", \"aerospace\", ...]. \n",
    "\n",
    "Our features will be based only the frequencies of terms in our vocabulary occuring in the e-mails (such an approach is called a *bag of words* approach, since we ignore the positions of the terms in the emails). The $j$-th feature is the number of times term $j$ in the vocabulary occurs in the email. If you are interested in further details on this model, you can see Chapters 6 and 13 in [Manning's Book](http://nlp.stanford.edu/IR-book/).\n",
    "\n",
    "I used the following classifiers in this problem:\n",
    "- sklearn.naive_bayes.BernoulliNB (Naive Bayes Classifier with Bernoulli Model)\n",
    "- sklearn.naive_bayes.MultinomialNB (Naive Bayes Classifier with Multinomial Model)\n",
    "- sklearn.svm.LinearSVC (Linear Support Vector Machine)\n",
    "- sklearn.linear_model.LogisticRegression (Logistic Regression)\n",
    "- sklearn.neighbors.KNeighborsClassifier (1-Nearest Neighbor Classifier)\n",
    "\n",
    "In the context of the Bernoulli Model for Naive Bayes, scikit-learn will binarize the features by interpretting the $j$-th feature to be $1$ if the $j$-th term in the vocabulary occurs in the email and $0$ otherwise. This is a categorical Naive Bayes model, with binary features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample Ham email is:\n",
    "\n",
    "    From nic@starflung.com  Mon Jun 24 17:06:54 2002\n",
    "    Return-Path: 7910726.0.27May2002215326@mp.opensrs.net\n",
    "    Delivery-Date: Tue May 28 02:53:28 2002\n",
    "    Received: from mp.opensrs.net (mp.opensrs.net [216.40.33.45]) by\n",
    "        dogma.slashnull.org (8.11.6/8.11.6) with ESMTP id g4S1rSe14718 for\n",
    "        <zzz@spamassassin.taint.org>; Tue, 28 May 2002 02:53:28 +0100\n",
    "    Received: (from popensrs@localhost) by mp.opensrs.net (8.9.3/8.9.3) id\n",
    "        VAA04361; Mon, 27 May 2002 21:53:26 -0400\n",
    "    Message-Id: <7910726.0.27May2002215326@mp.opensrs.net>\n",
    "    Date: Mon, 27 May 2002 21:53:26 -0500 (EST)\n",
    "    From: \"Starflung NIC\" <nic@starflung.com>\n",
    "    To: <zzz@spamassassin.taint.org>\n",
    "    Subject: Automated 30 day renewal reminder 2002-05-27\n",
    "    X-Keywords: \n",
    "\n",
    "    The following domains that are registered as belonging\n",
    "    to you are due to expire within the next 60 days. If\n",
    "    you would like to renew them, please contact\n",
    "    nic@starflung.com; otherwise they will be deactivated\n",
    "    and may be registered by another.\n",
    "\n",
    "\n",
    "    Domain Name, Expiry Date\n",
    "    nutmegclothing.com, 2002-06-26\n",
    "    \n",
    "    \n",
    "A sample Spam email is: \n",
    "\n",
    "    From jjj@mymail.dk  Fri Aug 23 11:03:31 2002\n",
    "    Return-Path: <jjj@mymail.dk>\n",
    "    Delivered-To: zzzz@localhost.example.com\n",
    "    Received: from localhost (localhost [127.0.0.1])\n",
    "        by phobos.labs.example.com (Postfix) with ESMTP id 478B54415C\n",
    "        for <zzzz@localhost>; Fri, 23 Aug 2002 06:02:57 -0400 (EDT)\n",
    "    Received: from mail.webnote.net [193.120.211.219]\n",
    "        by localhost with POP3 (fetchmail-5.9.0)\n",
    "        for zzzz@localhost (single-drop); Fri, 23 Aug 2002 11:02:57 +0100 (IST)\n",
    "    Received: from smtp.easydns.com (smtp.easydns.com [205.210.42.30])\n",
    "        by webnote.net (8.9.3/8.9.3) with ESMTP id IAA08912;\n",
    "        Fri, 23 Aug 2002 08:13:36 +0100\n",
    "    From: jjj@mymail.dk\n",
    "    Received: from mymail.dk (unknown [61.97.34.233])\n",
    "        by smtp.easydns.com (Postfix) with SMTP\n",
    "        id 7484A2F85C; Fri, 23 Aug 2002 03:13:31 -0400 (EDT)\n",
    "    Reply-To: <jjj@mymail.dk>\n",
    "    Message-ID: <008c61d64eed$6184e5d5$4bc22de3@udnugg>\n",
    "    To: bbr_hooten@yahoo.com\n",
    "    Subject: HELP WANTED.  WORK FROM HOME REPS.\n",
    "    MiME-Version: 1.0\n",
    "    Content-Type: text/plain;\n",
    "        charset=\"iso-8859-1\"\n",
    "    X-Priority: 3 (Normal)\n",
    "    X-MSMail-Priority: Normal\n",
    "    X-Mailer: Microsoft Outlook, Build 10.0.2616\n",
    "    Importance: Normal\n",
    "    Date: Fri, 23 Aug 2002 03:13:31 -0400 (EDT)\n",
    "    Content-Transfer-Encoding: 8bit\n",
    "\n",
    "    Help wanted.  We are a 14 year old fortune 500 company, that is\n",
    "    growing at a tremendous rate.  We are looking for individuals who\n",
    "    want to work from home.\n",
    "\n",
    "    This is an opportunity to make an excellent income.  No experience\n",
    "    is required.  We will train you.\n",
    "\n",
    "    So if you are looking to be employed from home with a career that has\n",
    "    vast opportunities, then go:\n",
    "\n",
    "    http://www.basetel.com/wealthnow\n",
    "\n",
    "    We are looking for energetic and self motivated people.  If that is you\n",
    "    than click on the link and fill out the form, and one of our\n",
    "    employement specialist will contact you.\n",
    "\n",
    "    To be removed from our link simple go to:\n",
    "\n",
    "    http://www.basetel.com/remove.html\n",
    "\n",
    "\n",
    "    1349lmrd5-948HyhJ3622xXiM0-290VZdq6044fFvN0-799hUsU07l50\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will load the data. Our dataset has a bit over 9000 emails, with about 25% of them being spam. We will use 50% of them as a training set, 25% of them as a validation set and 25% of them as a test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of emails\n",
    "spamfiles=glob.glob('./Data/Spam/*')\n",
    "hamfiles=glob.glob('./Data/Ham/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we will split the files into the training, validation and test sets.\n",
    "\n",
    "np.random.seed(seed=222017) # seed the RNG for repeatability\n",
    "\n",
    "fnames=np.asarray(spamfiles+hamfiles)\n",
    "nfiles=fnames.size\n",
    "labels=np.ones(nfiles)\n",
    "labels[len(spamfiles):]=-1\n",
    "\n",
    "# Randomly permute the files we have\n",
    "idx=np.random.permutation(nfiles)\n",
    "fnames=fnames[idx]\n",
    "labels=labels[idx]\n",
    "\n",
    "#Split the file names into which set they belong to\n",
    "tname=fnames[:int(nfiles/2)]\n",
    "trainlabels=labels[:int(nfiles/2)]\n",
    "vname=fnames[int(nfiles/2):int(nfiles*3/4)]\n",
    "vallabels=labels[int(nfiles/2):int(nfiles*3/4)]\n",
    "tename=fnames[int(3/4*nfiles):]\n",
    "testlabels=labels[int(3/4*nfiles):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Get our Bag of Words Features from the data\n",
    "bow = CountVectorizer(input='filename',encoding='iso-8859-1',binary=False)\n",
    "traindata=bow.fit_transform(tname)\n",
    "valdata=bow.transform(vname)\n",
    "testdata=bow.transform(tename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $100$ most and least common terms in the vocabulary are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 most common terms:  slashnull,dogma,click,not,request,ist,exmh,as,thu,wed,jmason,have,cnet,lists,or,are,mon,html,freshrpms,date,mailman,align,message,00,12,users,postfix,text,arial,type,bgcolor,rpm,ie,22,linux,version,be,taint,your,mailto,20,admin,table,sourceforge,content,color,jm,face,on,border,example,127,aug,gif,this,href,10,img,subject,src,09,nbsp,sep,it,that,0100,height,spamassassin,esmtp,is,size,xent,you,fork,tr,in,list,11,www,br,width,received,localhost,id,of,and,org,by,with,for,net,td,font,2002,from,3d,http,to,the,com \n",
      "\n",
      "100 least common terms:  ÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿÿó,snd_index,crittenden,vghhbmsgww91iezvcibzb3vyifrpbwusiefu,snd_major,criuqccibiqnijvkpqkraocgwoeadz9qikwzkkxzo94ow8jaq7kza2xlzdpk9fw07swhmk6zfx4k,crjowqpf7aywdqg8cke9fwakiatnugirizcja5ougeakmggdbyyditzrgcracagvtgzb05k6j8,vgh9,crlsbd,crlsca,vgggkvaadjknu,vgfliejvpgjypldvcmtvdxqgncbqywnrpc9ipjxicj4gidxm,cro19c3zkymlwxiwrclxwvnkbo,vgfby5giuasgpjnj9rtm42jaariaaabm77x,croatian,croatians,sndqbqy,crocodile,li4xiedyzwf0ifbyawnliq0kqsakmzawkybdb21iaw5lzcbszxrhawwgvmfs,li4uli4uli4umtegbw9udghzihbhc3nlzcb0agvuigl0igx1y2tpbhkgy2ft,li4uli4gjduwmcardqozli4uli4gjdusmdawicsgnc4uli4uicq1mcwwmdag,li4gjduwmcardqogicagicagicagicagicagicazli4uli4gjdusmdawicsn,sneakers,croininsimon,croissant,vgeo6v6s0zgo3luaiczcnu6krbnq,cronenberg,cronies,croninsimon,cronjobs,critiqued,vghhdcdzihjpz2h0isbjzib5b3ugagf2zsbjyxigcgf5bwvudhmsihjlbnqspgjypg0kicagicag,vghhdcdzptewmcbwzw9wbgugcmvzcg9uzgvkigfuzcbvcmrlcmvkifjlcg9y,criticize,libadm,criminologia,sncmulpw40hx1jbxrzu2fvmkcmq,cringe,lib_dp_tfcv,cripple,sncqcnkttecctgn5bvtz7vvkpqqrqc6zzxogcy7uv2qkdtio8kmkdrxvgcvw2pchmrvntc,cripples,vgkrcoy7cw8w9xdvzq4wnxiaebf5b3p5,cripssake,cription,vgkpyjuhocexvfofdf0x2butactlaascz19xa9tqovhex,vgk1b3zevjrnvte0izbf82iws9yunh3yk5vpxgbpcbuhjuc8n5fppfm4z87lg,vgjscoci,lhxni1x1kp8wpyeavftigutqlgjou,liars,liapoz,snd_cards_limit,cristine,snd_device_mode,vghlihjlyxnvbibmb3igdghlicjjyxnoiibpcybub3qgymvjyxvzzsb0aglz,vghlidaumiugcmvzcg9uc2ugdg8gdghhdcbpcyaxmdawig9yzgvycybmb3in,criterious,liam_mac_r,critically,criticalpath,criticise,criticisim,snd_id,liagvmlzaxq6igh0dha6ly93d3cuywzmb3jkywjszs1kb21haw5zlmnvbsb0,snd_amp_gpio,vgem05sx1zigu7,vgegbepycobcieacp8dabraqqlqsxgk7qs4gpkc1kpyajg7ghfgeeplsnwvlquljprycfqsqx1eb,vgd2xuaeribtqvopwbga5x,lhjrjtjznanygo9w4bbc6gy30qqlb2acfxdeaqogayslupkqkakg,lhjrfspwvo,vfz6ns9a5vreuanz6tizri9lq1fpmzgnwerse6ay,sneakier,lhhjy7cysohyngev7alcegggbvixwrbxfdo0fquldavryqs7hsqmtqylpatjvwlzou42krg8alzl,lhhgc8c93twndybyz87mljkwyefouktdiwbvstexwl62nhhiiglf4zz1xpwibtjykkug,crqwbmwcjgclmc6czzqmdgoamkyjpcocgazcw4azmsbiytvwiqzgbw7gaoabg0zqwfslciagauyb,crrlnrbxvkknrtigrmowkajbsgpwgb1axrqayy4tikaifdfmqjggbhoenbpqbgayh,lhgubu5p5bx,vfyy6x,crs4s4ulhldawv5gafss0r0r0ontigzc7,vfyxeuedwacdjxriumtvrkybkfketsuq,lhg,vfxmobf8xedxwcyr,lhnpm5q,crts,crua,vfxd1nktpwakkaa2qougeb08zabg4kk1kgumo3bsqi0xcjybhibogmsjgkcaz17nncogwlenelbt,crucially,lhfymtlomst3,vfwsmnpxx9lgc2lonue,crudely,lhc5a7swao1rvec8fzbwts8a75tfdkvcegdnwaqowj8ddeabcburni1mykhs0kieouosb2jhaijq,lhbxvzinmgtulpbgw,lhbxvzinmgq3jvc,lhapayggarbgmx88mijp8pe,crui,lhamo,lhaigood,lh6riid,crtyler1,snatches\n"
     ]
    }
   ],
   "source": [
    "counts=np.reshape(np.asarray(np.argsort(traindata.sum(axis=0))),-1)\n",
    "vocab=np.reshape(np.asarray(bow.get_feature_names()),-1)\n",
    "print (\"100 most common terms: \" , ','.join(str(s) for s in vocab[counts[-100:]]), \"\\n\")\n",
    "print (\"100 least common terms: \" , ','.join(str(s) for s in vocab[counts[:100]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will have our training data in `traindata` (with labels in `trainlabels`), validation data in `valdata` (with labels in `vallabels`) and test data in `testdata` (with labels in `testlabels`). The data is stored as a sparse scipy matrix (scipy.sparse.csr.csr_matrix), since we have a decent number of features (~100k), most of which are zero (~0.2% are non-zero), this allows storing the data in a few megabytes. Directly storing it as a numpy array would take around 8 gigabytes. Working with sparse data can make many algorithms run faster and use less storage. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I trained each of the following classifiers:\n",
    "- sklearn.naive_bayes.BernoulliNB (Naive Bayes Classifier with Bernoulli Model)\n",
    "- sklearn.naive_bayes.MultinomialNB (Naive Bayes Classifier with Multinomial Model)\n",
    "- sklearn.svm.LinearSVC (Linear Support Vector Machine)\n",
    "- sklearn.linear_model.LogisticRegression (Logistic Regression)\n",
    "- sklearn.neighbors.KNeighborsClassifier (as a 1-Nearest Neighbor Classifier)\n",
    "on the training data in `traindata` with corresponding labels `trainlabels`. Use the default parameters, unless otherwise noted.\n",
    "\n",
    "For each classifier:\n",
    "- Time it took to fit the classifier (i.e. call the .fit method) \n",
    "- Training Error \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB \n",
    "from sklearn.naive_bayes import MultinomialNB \n",
    "from sklearn.svm import LinearSVC \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB\n",
      "29.5 ms ± 4.37 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Training Error: 0.05541292255027813 \n",
      "\n",
      "Multinomial NB\n",
      "17.7 ms ± 476 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "Training Error: 0.01647411210954215 \n",
      "\n",
      "Linear SVC\n",
      "545 ms ± 6.53 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "Training Error: 0.0 \n",
      "\n",
      "Logistic Regression\n",
      "1.76 s ± 124 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "Training Error: 0.0 \n",
      "\n",
      "KNeighbors Classifier\n",
      "5.06 ms ± 317 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "Training error: 0.03465982028241335 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def classifierError(truelabels,estimatedlabels):\n",
    "    \n",
    "    error_counter = 0\n",
    "    \n",
    "    for i in range(0, truelabels.shape[0]):\n",
    "        \n",
    "        if(truelabels[i] != (estimatedlabels[i])):\n",
    "            error_counter += 1\n",
    "            \n",
    "    return (error_counter/truelabels.shape[0])\n",
    "\n",
    "clf_bernoulli = BernoulliNB()\n",
    "print(\"Bernoulli NB\")\n",
    "%timeit clf_bernoulli.fit(traindata, trainlabels)\n",
    "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)\n",
    "estimated_trainlabels = clf_bernoulli.predict(traindata)\n",
    "error = classifierError(trainlabels, estimated_trainlabels)\n",
    "print(\"Training Error:\", error,\"\\n\")\n",
    "\n",
    "clf_multinomial = MultinomialNB()\n",
    "print(\"Multinomial NB\")\n",
    "%timeit clf_multinomial.fit(traindata, trainlabels)\n",
    "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
    "estimated_trainlabels = clf_multinomial.predict(traindata)\n",
    "error = classifierError(trainlabels, estimated_trainlabels)\n",
    "print(\"Training Error:\", error,\"\\n\")\n",
    "\n",
    "clf_linearsvc = LinearSVC(random_state=0)\n",
    "print(\"Linear SVC\")\n",
    "%timeit clf_linearsvc.fit(traindata, trainlabels)\n",
    "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
    "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
    "     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,\n",
    "     verbose=0)\n",
    "estimated_trainlabels = clf_linearsvc.predict(traindata)\n",
    "error = classifierError(trainlabels, estimated_trainlabels)\n",
    "print(\"Training Error:\", error,\"\\n\")\n",
    "\n",
    "clf_logisticregression = LogisticRegression()\n",
    "print(\"Logistic Regression\")\n",
    "%timeit clf_logisticregression.fit(traindata, trainlabels)\n",
    "estimated_trainlabels = clf_logisticregression.predict(traindata)\n",
    "error = classifierError(trainlabels, estimated_trainlabels)\n",
    "print(\"Training Error:\", error,\"\\n\")\n",
    "\n",
    "clf_kneighbors = KNeighborsClassifier()\n",
    "print(\"KNeighbors Classifier\")\n",
    "%timeit clf_kneighbors.fit(traindata, trainlabels)\n",
    "estimated_trainlabels = clf_kneighbors.predict(valdata)\n",
    "error = classifierError(vallabels,estimated_trainlabels) \n",
    "print(\"Training error:\", error, \"\\n\")\n",
    "\n",
    "# clf_svc = SVC()\n",
    "# print (\"SVC\")\n",
    "# %timeit clf_svc.fit(traindata, trainlabels) \n",
    "# SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "#     decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
    "#     max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "#     tol=0.001, verbose=False)\n",
    "# estimated_trainlabels = clf_svc.predict(traindata)\n",
    "# error = classifierError(trainlabels, estimated_trainlabels)\n",
    "# print(\"Training Error:\", error,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ran each of the classifiers on the validation data:\n",
    "- sklearn.naive_bayes.BernoulliNB (Naive Bayes Classifier with Bernoulli Model)\n",
    "- sklearn.naive_bayes.BernoulliNB (Naive Bayes Classifier with Multiomial Model)\n",
    "- sklearn.svm.LinearSVC (Linear Support Vector Machine)\n",
    "- sklearn.linear_model.LogisticRegression (Logistic Regression)\n",
    "- sklearn.neighbors.KNeighborsClassifier (as a 1-Nearest Neighbor Classifier)\n",
    "on the training data in `traindata` with corresponding labels `trainlabels`. Use the default parameters, unless otherwise noted.\n",
    "\n",
    "For each classifier:\n",
    "- Store the labels it predicted as \\_\\_vallabels, where \\_\\_ is NB,MB,SVM,LR,NN respectively. \n",
    "- Time it took to run the classifier on the data \n",
    "- Validation Error \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Classifier with Bernoulli Model\n",
      "15.3 ms ± 170 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "Validation Error: 0.09584937954642704 \n",
      "\n",
      "Naive Bayes Classifier with Multinomial Model\n",
      "6.1 ms ± 141 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "Validation Error: 0.03123662815575524 \n",
      "\n",
      "Linear Suppport Vector Machine\n",
      "2.63 ms ± 42.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "Validation Error: 0.008130081300813009 \n",
      "\n",
      "Logistic Regression\n",
      "2.78 ms ± 305 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "Validation Error: 0.008130081300813009 \n",
      "\n",
      "1-Nearest Neighbor Classifier\n",
      "2.19 s ± 110 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "Validation error:\n",
      " 0.017115960633290545 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Naive Bayes Classifier with Bernoulli Model\")\n",
    "%timeit clf_bernoulli.predict(valdata)\n",
    "NB_vallabels = clf_bernoulli.predict(valdata)\n",
    "error = classifierError(vallabels, NB_vallabels)\n",
    "print(\"Validation Error:\", error,\"\\n\")\n",
    "\n",
    "print(\"Naive Bayes Classifier with Multinomial Model\")\n",
    "%timeit clf_multinomial.predict(valdata)\n",
    "MB_vallabels = clf_multinomial.predict(valdata)\n",
    "error = classifierError(vallabels, MB_vallabels)\n",
    "print(\"Validation Error:\", error,\"\\n\")\n",
    "\n",
    "print(\"Linear Suppport Vector Machine\")\n",
    "%timeit clf_linearsvc.predict(valdata)\n",
    "SVM_vallabels = clf_linearsvc.predict(valdata)\n",
    "error = classifierError(vallabels, SVM_vallabels)\n",
    "print(\"Validation Error:\", error,\"\\n\")\n",
    "\n",
    "print(\"Logistic Regression\")\n",
    "%timeit clf_logisticregression.predict(valdata)\n",
    "LR_vallabels = clf_logisticregression.predict(valdata)\n",
    "error = classifierError(vallabels, LR_vallabels)\n",
    "print(\"Validation Error:\", error,\"\\n\")\n",
    "\n",
    "print(\"1-Nearest Neighbor Classifier\")\n",
    "clf_kneighbors = neighbors.KNeighborsClassifier(n_neighbors=1)\n",
    "clf_kneighbors.fit(traindata, trainlabels)\n",
    "%timeit clf_kneighbors.predict(valdata)\n",
    "NN_vallabels = clf_kneighbors.predict(valdata)\n",
    "error = classifierError(vallabels,NN_vallabels) \n",
    "print(\"Validation error:\\n\", error, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take a more nuanced look at the type of errors made on a data set. The following function calculates a confusion matrix and some statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConfMatr(truelabels,estimatedlabels,classifiername):\n",
    "    # classifiername is a string, such as 'Naive Bayes (Bernoulli)'\n",
    "    cm=np.zeros((2,2))\n",
    "    cm[0,0]=np.sum(np.logical_and(truelabels==1,estimatedlabels==1)) # True Positives\n",
    "    cm[0,1]=np.sum(np.logical_and(truelabels==-1,estimatedlabels==1)) # False Positive\n",
    "    cm[1,0]=np.sum(np.logical_and(truelabels==1,estimatedlabels==-1)) # False Negative\n",
    "    cm[1,1]=np.sum(np.logical_and(truelabels==-1,estimatedlabels==-1)) # True Negatives\n",
    "    print (\"Classifier Name: %s\"% classifiername )\n",
    "    print (\"True Positives:\", cm[0,0], \"False Positive:\", cm[0,1])\n",
    "    print (\"False Negative:\", cm[1,0], \"True Negatives:\", cm[1,1])\n",
    "    print (\"True Positive Rate : \", cm[0,0]/np.sum(truelabels==1))\n",
    "    print (\"False Positive Rate: \", cm[0,1]/np.sum(truelabels==-1))\n",
    "    print (\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run ConfMatr using the validation labels and their estimates for all the classifiers we've used in this problem. **(5 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Name: Naive Bayes Classifier with Bernoulli Model\n",
      "True Positives: 389.0 False Positive: 11.0\n",
      "False Negative: 213.0 True Negatives: 1724.0\n",
      "True Positive Rate :  0.646179401993\n",
      "False Positive Rate:  0.00634005763689\n",
      "---\n",
      "Classifier Name: Naive Bayes Classifier with Multinomial Model\n",
      "True Positives: 538.0 False Positive: 9.0\n",
      "False Negative: 64.0 True Negatives: 1726.0\n",
      "True Positive Rate :  0.893687707641\n",
      "False Positive Rate:  0.00518731988473\n",
      "---\n",
      "Classifier Name: Linear Suppport Vector Machine\n",
      "True Positives: 593.0 False Positive: 10.0\n",
      "False Negative: 9.0 True Negatives: 1725.0\n",
      "True Positive Rate :  0.985049833887\n",
      "False Positive Rate:  0.00576368876081\n",
      "---\n",
      "Classifier Name: Logistic Regression\n",
      "True Positives: 593.0 False Positive: 10.0\n",
      "False Negative: 9.0 True Negatives: 1725.0\n",
      "True Positive Rate :  0.985049833887\n",
      "False Positive Rate:  0.00576368876081\n",
      "---\n",
      "Classifier Name: 1-Nearest Neighbor Classifier\n",
      "True Positives: 576.0 False Positive: 14.0\n",
      "False Negative: 26.0 True Negatives: 1721.0\n",
      "True Positive Rate :  0.956810631229\n",
      "False Positive Rate:  0.00806916426513\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Put your code here\n",
    "ConfMatr(vallabels, NB_vallabels,\"Naive Bayes Classifier with Bernoulli Model\")\n",
    "ConfMatr(vallabels, MB_vallabels,\"Naive Bayes Classifier with Multinomial Model\")\n",
    "ConfMatr(vallabels, SVM_vallabels,\"Linear Suppport Vector Machine\")\n",
    "ConfMatr(vallabels, LR_vallabels,\"Logistic Regression\")\n",
    "ConfMatr(vallabels, NN_vallabels,\"1-Nearest Neighbor Classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[True positive rate is how many how many spam emails actually get classified as spam email.\n",
    "The higher the true positive rate, the more spam gets filtered out. Therefore, we would want a true positive rate.\n",
    "False positive rate is how many ham emails get classified as spam email.\n",
    "We want the false positive rate to be extremely low, because we don't want to miss any ham email which may be of importance. ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[The classifier I would choose for this problem is a linear SVM. The linear SVM has the highest true positive rate and one of the least false positive rates. For detecting spam in the UofI email system, I would firstly have a large bunch of keywords to check for in emails, just like the problem we just solved above. I would also have a report spam button in user's mailboxes to identify new spam threats and add keywords from the new spam threats to my existing keywords set. The report spam button is one of the biggest factors why Gmail is so successful in preventing spam. ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: 0.00641573994867408 \n",
      "\n",
      "Classifier Name: Linear Suppport Vector Machine\n",
      "True Positives: 613.0 False Positive: 8.0\n",
      "False Negative: 7.0 True Negatives: 1710.0\n",
      "True Positive Rate :  0.988709677419\n",
      "False Positive Rate:  0.0046565774156\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "estimated_testlabels = clf_linearsvc.predict(testdata)\n",
    "error = classifierError(testlabels, estimated_testlabels)\n",
    "print(\"Test Error:\", error,\"\\n\")\n",
    "\n",
    "ConfMatr(testlabels, estimated_testlabels,\"Linear Suppport Vector Machine\")\n",
    "\n",
    "# clf_linearsvc.predict(valdata)\n",
    "# SVM_vallabels = clf_linearsvc.predict(valdata)\n",
    "# error = classifierError(vallabels, SVM_vallabels)\n",
    "# print(\"Validation Error:\", error,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Test Error: 0.00641573994867408 \n",
    "\n",
    "Classifier Name: Linear Suppport Vector Machine\n",
    "True Positives: 613.0 False Positive: 8.0\n",
    "False Negative: 7.0 True Negatives: 1710.0\n",
    "True Positive Rate :  0.988709677419\n",
    "False Positive Rate:  0.0046565774156\n",
    "\n",
    "The false positive rate for the test data is actually lower than the false positive rate for the validation data. This is a good thing because this means that lesser ham emails are classified as spam emails.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Cross-Validation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a function which implements $5$-fold cross-validation to estimate the error of a classifier with cross-validation with the 0,1-loss for k-Nearest Neighbors (kNN). \n",
    "\n",
    "Input:\n",
    "* A (N,d) numpy.ndarray of training data, trainData (with N divisible by 5)\n",
    "* A length $N$ numpy.ndarray of training labels, trainLabels\n",
    "* A number $k$, for which cross-validated error estimates will be outputted for $1,\\ldots,k$\n",
    "\n",
    "The output will be a vector (represented as a numpy.ndarray) err, such that err[i] is the cross-validated estimate of using i neighbors (err will be of length $k+1$; the zero-th component of the vector will be meaningless). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossValidationkNN(trainData,trainLabels,k):\n",
    "\n",
    "    # Error vector\n",
    "    err = np.zeros(k + 1)\n",
    "\n",
    "    # Number of data points\n",
    "    n = trainData.shape[0]\n",
    "    n /= 5\n",
    "    \n",
    "    copy_trainData = trainData\n",
    "    copy_trainLabels = trainLabels\n",
    "    \n",
    "    for i in range(1, k + 1):\n",
    "        \n",
    "        err_holder = 0\n",
    "        for j in range(1, (5 + 1)):\n",
    "            \n",
    "            # Extract the validation set\n",
    "            validation_set_data = trainData[int(((j - 1) * n)):int((j * n))]\n",
    "            #print(validation_set_data.shape[0])\n",
    "            validation_set_labels = trainLabels[int(((j - 1) * n)):int((j * n))]\n",
    "            #print(validation_set_labels.shape[0])\n",
    "\n",
    "            #arr = np.delete(arr, np.s_[0:2], 0)\n",
    "            training_set_data = np.delete(copy_trainData, np.s_[int(((j - 1) * n)):int((j * n))], 0)\n",
    "            #print(training_set_data.shape[0])\n",
    "            training_set_labels = np.delete(copy_trainLabels, np.s_[int(((j - 1) * n)):int((j * n))], 0)\n",
    "            #print(training_set_labels.shape[0])\n",
    "            #print(i)\n",
    "            # Reset training set holders\n",
    "            copy_trainData = trainData\n",
    "            copy_trainLabels = trainLabels\n",
    "\n",
    "            # Perform training\n",
    "            neigh = neighbors.KNeighborsClassifier(n_neighbors=i, algorithm='brute')\n",
    "            neigh.fit(training_set_data, training_set_labels)\n",
    "            estimatedvallabels = neigh.predict(validation_set_data)\n",
    "            #vallabels_cross_validation = vallabels[int(((j - 1) * n)):int((j * n))]\n",
    "            err_holder += classifierError(validation_set_labels,estimatedvallabels)\n",
    "        err[i] = err_holder/5\n",
    "        #err[i] = neigh.score()\n",
    "    return err\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will load some data (acquired from <a href=\"http://www.cs.ubc.ca/~murphyk/\">K.P. Murphy</a>'s <a href=\"https://github.com/probml/pmtk3\"> PMTK tookit</a>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem2_tmp= genfromtxt('Data/p2.csv', delimiter=',')\n",
    "\n",
    "# Randomly reorder the data\n",
    "np.random.seed(seed=2217) # seed the RNG for repeatability\n",
    "idx=np.random.permutation(problem2_tmp.shape[0])\n",
    "problem2_tmp=problem2_tmp[idx]\n",
    "\n",
    "#The training data which you will use is called \"traindata\"\n",
    "traindata=problem2_tmp[:200,:2]\n",
    "#The training labels are in \"labels\"\n",
    "trainlabels=problem2_tmp[:200,2]\n",
    "\n",
    "#The test data which you will use is called \"testdata\" with labels \"testlabels\"\n",
    "testdata=problem2_tmp[200:,:2]\n",
    "testlabels=problem2_tmp[200:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the cross-validation error versus number of neighbors for $1,\\ldots,30$ neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x114076630>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113b77b38>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "index = np.arange(0,31)\n",
    "# 5-fold Cross validation\n",
    "err = crossValidationkNN(traindata,trainlabels,30)\n",
    "plt.plot(index[1:], err[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of neighbors which minimizes the cross-validation error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Cross Validation Error:  0.175 \n",
      "\n",
      "Number of neighbors:  14 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "min_error = np.amin(err[1:])\n",
    "min_neighbors = np.argmin(err[1:])\n",
    "print(\"Minimum Cross Validation Error: \", min_error, \"\\n\")\n",
    "print(\"Number of neighbors: \", min_neighbors + 1, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Minimum Cross Validation Error:  0.175\n",
    "\n",
    "Number of neighbors:  14]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trained a kNN model on the whole training data using the minimum number of neighbors found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error:  0.214 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "neigh_train = neighbors.KNeighborsClassifier(n_neighbors=14, algorithm='brute')\n",
    "neigh_train.fit(traindata, trainlabels)\n",
    "estimatedtestlabels_knn = neigh_train.predict(testdata)\n",
    "err_knn = classifierError(testlabels,estimatedtestlabels_knn)\n",
    "print(\"Test Error: \", err_knn, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Test Error:  0.214\n",
    "This test error is much higher than the previous higher cross validation error.]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting Cancer with SVMs and Logistic Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider the [Breast Cancer Wisconsin Data Set](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29) from \n",
    "W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on Electronic Imaging: Science and Technology, volume 1905, pages 861-870, San Jose, CA, 1993. \n",
    "\n",
    "The authors diagnosed people by characterizing 3 cell nuclei per person extracted from the breast (pictures [here](http://web.archive.org/web/19970225174429/http://www.cs.wisc.edu/~street/images/)), each with 10 features (for a 30-dimensional feature space):\n",
    "\n",
    "1. radius (mean of distances from center to points on the perimeter) \n",
    "\n",
    "2. texture (standard deviation of gray-scale values) \n",
    "\n",
    "3. perimeter \n",
    "\n",
    "4. area \n",
    "\n",
    "5. smoothness (local variation in radius lengths) \n",
    "\n",
    "6. compactness (perimeter^2 / area - 1.0) \n",
    "\n",
    "7. concavity (severity of concave portions of the contour) \n",
    "\n",
    "8. concave points (number of concave portions of the contour) \n",
    "\n",
    "9. symmetry \n",
    "\n",
    "10. fractal dimension (\"coastline approximation\" - 1)\n",
    "\n",
    "and classified the sample into one of two classes: Malignant ($+1$) or Benign ($-1$). You can read the original paper for more on what these features mean.\n",
    "\n",
    "I attempted to classify if a sample is Malignant or Benign using Support Vector Machines, as well as Logistic Regression. Since we don't have all that much data, I used 10-fold cross-validation to tune our parameters for our SVMs and Logistic Regression. We use 90% of the data for training, and 10% for testing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load the data. We will use scikit-learn's train test split function to split the data. The data is scaled for reasons outlined <a href=\"http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf\">here</a>. In short, it helps avoid some numerical issues and avoids some problems with certain features which are typically large affecting the SVM optimization problem unfairly compared to features which are typically small. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cancer = genfromtxt('Data/wdbc.csv', delimiter=',')\n",
    "\n",
    "np.random.seed(seed=282017) # seed the RNG for repeatability\n",
    "idx=np.random.permutation(cancer.shape[0])\n",
    "cancer=cancer[idx]\n",
    "\n",
    "cancer_features=cancer[:,1:]\n",
    "cancer_labels=cancer[:,0]\n",
    "\n",
    "#The training data is in data_train with labels label_train. \n",
    "# The test data is in data_test with labels label_test.\n",
    "data_train, data_test, label_train, label_test = train_test_split(cancer_features,cancer_labels,test_size=0.1,random_state=292017)\n",
    "\n",
    "# Rescale the training data and scale the test data correspondingly\n",
    "scaler=MinMaxScaler(feature_range=(-1,1))\n",
    "data_train=scaler.fit_transform(data_train) #Note that the scaling is determined solely via the training data!\n",
    "data_test=scaler.transform(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use cross-validation to select a value of $C$ for a linear SVM (sklearn.svm.LinearSVC) by varying $C$ from $2^{-5},2^{-4},\\ldots,2^{15}$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: 0.03125\n",
      "Cross Validation Error: 0.0349924585219 \n",
      "\n",
      "C: 0.0625\n",
      "Cross Validation Error: 0.0329939668175 \n",
      "\n",
      "C: 0.125\n",
      "Cross Validation Error: 0.027149321267 \n",
      "\n",
      "C: 0.25\n",
      "Cross Validation Error: 0.0291478129713 \n",
      "\n",
      "C: 0.5\n",
      "Cross Validation Error: 0.0330693815988 \n",
      "\n",
      "C: 1\n",
      "Cross Validation Error: 0.0311463046757 \n",
      "\n",
      "C: 2\n",
      "Cross Validation Error: 0.0311840120664 \n",
      "\n",
      "C: 4\n",
      "Cross Validation Error: 0.0292609351433 \n",
      "\n",
      "C: 8\n",
      "Cross Validation Error: 0.0292609351433 \n",
      "\n",
      "C: 16\n",
      "Cross Validation Error: 0.0311463046757 \n",
      "\n",
      "C: 32\n",
      "Cross Validation Error: 0.0368778280543 \n",
      "\n",
      "C: 64\n",
      "Cross Validation Error: 0.0351055806938 \n",
      "\n",
      "C: 128\n",
      "Cross Validation Error: 0.0351825037707 \n",
      "\n",
      "C: 256\n",
      "Cross Validation Error: 0.0429487179487 \n",
      "\n",
      "C: 512\n",
      "Cross Validation Error: 0.0467963800905 \n",
      "\n",
      "C: 1024\n",
      "Cross Validation Error: 0.0466440422323 \n",
      "\n",
      "C: 2048\n",
      "Cross Validation Error: 0.052641025641 \n",
      "\n",
      "C: 4096\n",
      "Cross Validation Error: 0.0371432880845 \n",
      "\n",
      "C: 8192\n",
      "Cross Validation Error: 0.0428355957768 \n",
      "\n",
      "C: 16384\n",
      "Cross Validation Error: 0.0469095022624 \n",
      "\n",
      "C: 32768\n",
      "Cross Validation Error: 0.0465309200603 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "X = data_train\n",
    "y = label_train\n",
    "for i in range(-5, 16):\n",
    "    val = 2**(i)\n",
    "    print(\"C:\", val)\n",
    "    svc = svm.LinearSVC(C=val)\n",
    "#lasso = linear_model.Lasso()\n",
    "    cross_validation_error = 1 - mean(cross_val_score(svc, X, y, cv=10))  \n",
    "    print(\"Cross Validation Error:\",cross_validation_error, \"\\n\")\n",
    "#print(cross_val_score(svc, X, y, cv=10))\n",
    "#cross_val_score(estimator, X, y=None, groups=None, scoring=None, cv=10, n_jobs=1, verbose=0, fit_params=None, pre_dispatch=‘2*n_jobs’)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[C: 0.125\n",
    "Cross Validation Error: 0.027149321267]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I experimented with using kernels in an SVM, particularly the Gaussian RBF kernel (in sklearn.svm.SVC). The SVM has two parameters to tune in this case: $C$ (as before), and $\\gamma$, which is a parameter in the RBF. \n",
    "\n",
    "Use cross-validation to select parameters $(C,\\gamma)$ by searching varying $(C,\\gamma)$ over $C=2^{-5},2^{-4},\\ldots,2^{15}$ and $\\gamma=2^{-15},\\ldots,2^{3}$ \n",
    "\n",
    "This procedure is known as a *grid search*.We are using a fairly coarse grid for this problem, but one could use a finer grid once the rough range of good parameters is known (rather than starting with a fine grid, which would waste a lot of time). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Error: 0.0194570135747 \n",
      "\n",
      "C: 8 \n",
      "\n",
      "Gamma: 0.125 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mini = 1\n",
    "# Loop for C\n",
    "for i in range(-5, 16):\n",
    "    \n",
    "    # Loop for Gamma\n",
    "    for j in range(-15, 4):\n",
    "        \n",
    "        val = 2**(i)\n",
    "        #print(\"C:\", val)\n",
    "        gam = 2**(j)\n",
    "        #print(\"Gamma:\", gam)\n",
    "        svc = svm.SVC(C=val, gamma=gam, kernel='rbf')\n",
    "        #lasso = linear_model.Lasso()\n",
    "        \n",
    "        cross_validation_error = 1 - mean(cross_val_score(svc, X, y, cv=10))\n",
    "        \n",
    "        if (cross_validation_error < mini):\n",
    "            \n",
    "            # Store C, gamma indices\n",
    "            i_val = i\n",
    "            j_gam = j\n",
    "            mini = cross_validation_error\n",
    "            \n",
    "print(\"Cross Validation Error:\",mini, \"\\n\")\n",
    "print(\"C:\", 2**(i_val), \"\\n\")\n",
    "print(\"Gamma:\", 2**(j_gam), \"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Cross Validation Error: 0.0194570135747 \n",
    "\n",
    "C: 8 \n",
    "\n",
    "Gamma: 0.125\n",
    "\n",
    "These values of C, Gamma minimise the validation error]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ogistic Regression normally has a regularizer parameter to promote stability. Scikit-learn calls this parameter $C$ (which is like $\\lambda^{-1}$ in the notes); see the [LibLinear](http://www.csie.ntu.edu.tw/~cjlin/papers/liblinear.pdf) documentation for the exact meaning of $C$. \n",
    "\n",
    "Use cross-validation to select a value of $C$ for logistic regression (sklearn.linear_model.LogisticRegression) by varying $C$ from $2^{-14},2^{-4},\\ldots,2^{14}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Error: 0.027149321267 \n",
      "\n",
      "C: 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "X = data_train\n",
    "y = label_train\n",
    "min = 1\n",
    "\n",
    "for i in range(-14, 15):\n",
    "    val = 2**(i)\n",
    "    #print(\"C:\", val)\n",
    "    lr = linear_model.LogisticRegression(C=val)\n",
    "#lasso = linear_model.Lasso()\n",
    "    cross_validation_error = 1 - mean(cross_val_score(lr, X, y, cv=10))  \n",
    "    #print(\"Cross Validation Error:\",cross_validation_error, \"\\n\")\n",
    "    if (cross_validation_error < min):\n",
    "        min = cross_validation_error\n",
    "        i_val = i\n",
    "        \n",
    "print(\"Cross Validation Error:\", min, \"\\n\")\n",
    "print(\"C:\", 2**(i_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Cross Validation Error: 0.027149321267 \n",
    "\n",
    "C: 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Out of all the classifiers tested, I would pick Linear SVM. It has the least cross-validation error. Linear SVM is also easier to train. Gaussian RBF takes a long time to compute.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I trained the classifier selected above on the whole training set and estimated the prediction error using the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Error: 0.017543859649122806 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clf_linearsvc = LinearSVC(random_state=0)\n",
    "\n",
    "clf_linearsvc.fit(data_train, label_train)\n",
    "LinearSVC(C=0.125, class_weight=None, dual=True, fit_intercept=True,\n",
    "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
    "     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,\n",
    "     verbose=0)\n",
    "estimated_test_labels = clf_linearsvc.predict(data_test)\n",
    "error = classifierError(label_test, estimated_test_labels)\n",
    "print(\"Prediction Error:\", error,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Prediction Error: 0.017543859649122806\n",
    "The prediction error is smaller than the cross-validation error]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[0-1 loss might not be the most appropriate performance measure. If you are suffering from a disease and it does not get caught it can be a huge problem. If you are not suffering from a disease and it gets reported its not a big deal. Therefore, we need some quantification for a a false positive and a false negative.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
